diff --git a/linux-2.6.34.7/arch/x86/include/asm/unistd_32.h b/linux-2.6.34.7/arch/x86/include/asm/unistd_32.h
index beb9b5f..b9a0c32 100644
--- a/linux-2.6.34.7/arch/x86/include/asm/unistd_32.h
+++ b/linux-2.6.34.7/arch/x86/include/asm/unistd_32.h
@@ -343,10 +343,12 @@
 #define __NR_rt_tgsigqueueinfo	335
 #define __NR_perf_event_open	336
 #define __NR_recvmmsg		337
+#define __NR_get_slob_amt_free	338
+#define __NR_get_slob_amt_claimed	339
 
 #ifdef __KERNEL__
 
-#define NR_syscalls 338
+#define NR_syscalls 340
 
 #define __ARCH_WANT_IPC_PARSE_VERSION
 #define __ARCH_WANT_OLD_READDIR
diff --git a/linux-2.6.34.7/arch/x86/kernel/syscall_table_32.S b/linux-2.6.34.7/arch/x86/kernel/syscall_table_32.S
index 8b37293..57b7089 100644
--- a/linux-2.6.34.7/arch/x86/kernel/syscall_table_32.S
+++ b/linux-2.6.34.7/arch/x86/kernel/syscall_table_32.S
@@ -337,3 +337,5 @@ ENTRY(sys_call_table)
 	.long sys_rt_tgsigqueueinfo	/* 335 */
 	.long sys_perf_event_open
 	.long sys_recvmmsg
+	.long sys_get_slob_amt_free		/* 338 */
+	.long sys_get_slob_amt_claimed	/* 339 */
diff --git a/linux-2.6.34.7/mm/slob.c b/linux-2.6.34.7/mm/slob.c
index 837ebd6..514eee6 100644
--- a/linux-2.6.34.7/mm/slob.c
+++ b/linux-2.6.34.7/mm/slob.c
@@ -1,4 +1,19 @@
 /*
+* CS 411, Group 13
+* James Admire, Dwight Trahin, Daniel Urbanski, Lewis Valentine
+*
+* Summary of changes:
+* Added a function fix_not_best that puts a block back into
+* its slob page
+*
+* Added some conditionals and messed with the control in
+* slob_page_alloc so that the block allocated should be the
+* best fit block on the page.
+*
+* (add your own changes here)
+*/
+
+/*
  * SLOB Allocator: Simple List Of Blocks
  *
  * Matt Mackall <mpm@selenic.com> 12/30/03
@@ -68,6 +83,7 @@
 #include <linux/list.h>
 #include <linux/kmemtrace.h>
 #include <linux/kmemleak.h>
+#include <linux/linkage.h>
 #include <asm/atomic.h>
 
 /*
@@ -84,6 +100,11 @@ typedef s16 slobidx_t;
 typedef s32 slobidx_t;
 #endif
 
+/*Globals for sys calls*/
+static long pageClaim = 0;
+static long assignmentClaim = 0;
+
+
 struct slob_block {
 	slobidx_t units;
 };
@@ -261,15 +282,87 @@ static void slob_free_pages(void *b, int order)
 		current->reclaim_state->reclaimed_slab += 1 << order;
 	free_pages((unsigned long)b, order);
 }
+/*
+* fix_not_best - Unallocates a block that was the best from a page, but not the overall best
+* @block: the pointer to a block to unallocate
+* @size:  the size of the current block attempting to be allocated
+*
+* Mostly copied from slob_free below.  Removed the lock and got rid of the case where size = PAGE_SIZE.
+*/
+static void fix_not_best(void *block, size_t size)
+{
+	struct slob_page *sp;
+	slob_t *prev, *next, *b = (slob_t *)block;
+	slobidx_t units;
+
+	early_printk("4");
+	if (unlikely(ZERO_OR_NULL_PTR(block)))
+		return;
+	BUG_ON(!size);
+
+	sp = slob_page(block);
+	units = SLOB_UNITS(size);
+
+	if (!slob_page_free(sp)) {
+		/* This slob page is about to become partially free. Easy! */
+		sp->units = units;
+		sp->free = b;
+		set_slob(b, units,
+			(void *)((unsigned long)(b +
+					SLOB_UNITS(PAGE_SIZE)) & PAGE_MASK));
+		set_slob_page_free(sp, &free_slob_small);
+		goto out;
+	}
+
+	/*
+	 * Otherwise the page is already partially free, so find reinsertion
+	 * point.
+	 */
+	sp->units += units;
+
+	if (b < sp->free) {
+		if (b + units == sp->free) {
+			units += slob_units(sp->free);
+			sp->free = slob_next(sp->free);
+		}
+		set_slob(b, units, sp->free);
+		sp->free = b;
+	} else {
+		prev = sp->free;
+		next = slob_next(prev);
+		while (b > next) {
+			prev = next;
+			next = slob_next(prev);
+		}
+
+		if (!slob_last(prev) && b + units == next) {
+			units += slob_units(next);
+			set_slob(b, units, slob_next(next));
+		} else
+			set_slob(b, units, next);
+
+		if (prev + slob_units(prev) == b) {
+			units = slob_units(b) + slob_units(prev);
+			set_slob(prev, units, slob_next(b));
+		} else
+			set_slob(prev, slob_units(prev), b);
+	}
+out:
+	return;
+}
+
 
 /*
- * Allocate a slob block within a given slob_page sp.
+ * Allocate the best fit slob block within a given slob_page sp.
+ * @best_size: a pointer with the current size of the best block.
  */
-static void *slob_page_alloc(struct slob_page *sp, size_t size, int align)
+static void *slob_page_alloc(struct slob_page *sp, size_t size, int align, int *best_size)
 {
-	slob_t *prev, *cur, *aligned = NULL;
+	slob_t *prev, *cur, *best, *bests_prev, *aligned = NULL;
 	int delta = 0, units = SLOB_UNITS(size);
-
+	best = bests_prev = NULL;
+	
+	early_printk("2");
 	for (prev = NULL, cur = sp->free; ; prev = cur, cur = slob_next(cur)) {
 		slobidx_t avail = slob_units(cur);
 
@@ -277,8 +370,10 @@ static void *slob_page_alloc(struct slob_page *sp, size_t size, int align)
 			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
 			delta = aligned - cur;
 		}
-		if (avail >= units + delta) { /* room enough? */
+
+		if (avail >= units + delta && (!best_size || avail < *best_size)) { /* room enough? AND better than best*/
 			slob_t *next;
+			*best_size = avail;
 
 			if (delta) { /* need to fragment head to align? */
 				next = slob_next(cur);
@@ -291,25 +386,62 @@ static void *slob_page_alloc(struct slob_page *sp, size_t size, int align)
 
 			next = slob_next(cur);
 			if (avail == units) { /* exact fit? unlink. */
-				if (prev)
+				if (best && !bests_prev) {
+				    set_slob(best, units + slob_units(sp->free), slob_next(sp->free));
+					sp->free = best;
+				}else if (best && bests_prev) {
+					if (best + units == slob_next(bests_prev))
+                        set_slob(best, units + slob_units(slob_next(bests_prev)), slob_next(slob_next(bests_prev)));
+					else
+                        set_slob(best, units, slob_next(bests_prev));
+
+					if (bests_prev + slob_units(bests_prev) == best)
+						set_slob(bests_prev, slob_units(bests_prev) + slob_units(best), slob_next(best));
+					else
+						set_slob(bests_prev, slob_units(bests_prev), best);
+				}
+
+				if (prev) {
 					set_slob(prev, slob_units(prev), next);
+					bests_prev = prev;
+				}
 				else
 					sp->free = next;
+				best = cur;
 			} else { /* fragment */
-				if (prev)
+				if (best && !bests_prev) {
+				    set_slob(best, units + slob_units(sp->free), slob_next(sp->free));
+					sp->free = best;
+				}else if (best && bests_prev) {
+				    if (best + units == slob_next(bests_prev))
+                        set_slob(best, units + slob_units(slob_next(bests_prev)), slob_next(slob_next(bests_prev)));
+                    else
+                        set_slob(best, units, slob_next(bests_prev));
+
+					if (bests_prev + slob_units(bests_prev) == best)
+						set_slob(bests_prev, slob_units(bests_prev) + slob_units(best), slob_next(best));
+					else
+						set_slob(bests_prev, slob_units(bests_prev), best);
+				}
+
+				if (prev) {
 					set_slob(prev, slob_units(prev), cur + units);
-				else
+					bests_prev = prev;
+				}else
 					sp->free = cur + units;
 				set_slob(cur + units, avail - units, next);
+				best = cur;
 			}
 
+		}
+
+		if (slob_last(cur)) {
 			sp->units -= units;
 			if (!sp->units)
 				clear_slob_page_free(sp);
-			return cur;
+			return best;
+
 		}
-		if (slob_last(cur))
-			return NULL;
 	}
 }
 
@@ -318,71 +450,72 @@ static void *slob_page_alloc(struct slob_page *sp, size_t size, int align)
  */
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
-	struct slob_page *sp;
-	struct list_head *prev;
-	struct list_head *slob_list;
-	slob_t *b = NULL;
-	unsigned long flags;
+    struct slob_page *sp;
+    slob_t *best = NULL;
+    struct list_head *slob_list;
+    slob_t *b = NULL;
+    unsigned long flags;
+    int tmp = SLOB_UNITS(PAGE_SIZE) + 1;
+    int score = 0;
 
-	if (size < SLOB_BREAK1)
-		slob_list = &free_slob_small;
-	else if (size < SLOB_BREAK2)
-		slob_list = &free_slob_medium;
-	else
-		slob_list = &free_slob_large;
+    slob_list = &free_slob_small;
 
-	spin_lock_irqsave(&slob_lock, flags);
-	/* Iterate through each partially free page, try to find room */
-	list_for_each_entry(sp, slob_list, list) {
+    spin_lock_irqsave(&slob_lock, flags);
+
+    /* Iterate through each partially free page, try to find room */
+    list_for_each_entry(sp, slob_list, list) {
 #ifdef CONFIG_NUMA
-		/*
-		 * If there's a node specification, search for a partial
-		 * page with a matching node id in the freelist.
-		 */
-		if (node != -1 && page_to_nid(&sp->page) != node)
-			continue;
+        /*
+         * If there's a node specification, search for a partial
+         * page with a matching node id in the freelist.
+         */
+        if (node != -1 && page_to_nid(&sp->page) != node)
+            continue;
 #endif
-		/* Enough room on this page? */
-		if (sp->units < SLOB_UNITS(size))
-			continue;
-
-		/* Attempt to alloc */
-		prev = sp->list.prev;
-		b = slob_page_alloc(sp, size, align);
-		if (!b)
-			continue;
-
-		/* Improve fragment distribution and reduce our average
-		 * search time by starting our next search here. (see
-		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
-	}
-	spin_unlock_irqrestore(&slob_lock, flags);
-
-	/* Not enough space: must allocate a new page */
-	if (!b) {
-		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
-		if (!b)
-			return NULL;
-		sp = slob_page(b);
-		set_slob_page(sp);
-
-		spin_lock_irqsave(&slob_lock, flags);
-		sp->units = SLOB_UNITS(PAGE_SIZE);
-		sp->free = b;
-		INIT_LIST_HEAD(&sp->list);
-		set_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));
-		set_slob_page_free(sp, slob_list);
-		b = slob_page_alloc(sp, size, align);
-		BUG_ON(!b);
-		spin_unlock_irqrestore(&slob_lock, flags);
+        /* Enough room on this page? */
+        if (sp->units < SLOB_UNITS(size))
+            continue;
+
+        b = slob_page_alloc(sp, size, align, &tmp);
+    	early_printk("1");
+
+        if ( (!best && b ) ||  score > tmp )
+        {
+	    if (best)
+	    	fix_not_best(best, size);
+            best = b;
+            score = tmp;
+        }else{
+		if (b)
+			fix_not_best(b, size);
 	}
-	if (unlikely((gfp & __GFP_ZERO) && b))
-		memset(b, 0, size);
-	return b;
+    }
+    spin_unlock_irqrestore(&slob_lock, flags);
+
+    /* Not enough space: must allocate a new page */
+    if (!best) {
+        pageClaim = pageClaim + PAGE_SIZE; //page allocated
+        b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
+        if (!b)
+            return NULL;
+        sp = slob_page(b);
+        set_slob_page(sp);
+
+        spin_lock_irqsave(&slob_lock, flags);
+        sp->units = SLOB_UNITS(PAGE_SIZE);
+        sp->free = b;
+        INIT_LIST_HEAD(&sp->list);
+        set_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));
+        set_slob_page_free(sp, slob_list);
+        b = slob_page_alloc(sp, size, align, NULL);
+        BUG_ON(!b);
+	best = b;
+        spin_unlock_irqrestore(&slob_lock, flags);
+    }
+    if (unlikely((gfp & __GFP_ZERO) && b))
+        memset(b, 0, size);
+    assignmentClaim = assignmentClaim + size;
+    return best;
 }
 
 /*
@@ -398,9 +531,9 @@ static void slob_free(void *block, int size)
 	if (unlikely(ZERO_OR_NULL_PTR(block)))
 		return;
 	BUG_ON(!size);
-
 	sp = slob_page(block);
 	units = SLOB_UNITS(size);
+	assignmentClaim = assignmentClaim - size; //assigned memory freed
 
 	spin_lock_irqsave(&slob_lock, flags);
 
@@ -412,6 +545,7 @@ static void slob_free(void *block, int size)
 		clear_slob_page(sp);
 		free_slob_page(sp);
 		slob_free_pages(b, 0);
+		pageClaim = pageClaim - PAGE_SIZE; //assigned page freed
 		return;
 	}
 
@@ -697,3 +831,13 @@ void __init kmem_cache_init_late(void)
 {
 	/* Nothing to do */
 }
+
+asmlinkage long sys_get_slob_amt_free(void)
+{
+	return pageClaim - assignmentClaim; //return the size of the pages minus the amount we've given away, the amount free
+}
+asmlinkage long sys_get_slob_amt_claimed(void)
+{
+	return pageClaim; //Return the size of the allocated pages.
+}
+
